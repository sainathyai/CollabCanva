Oct 15, 2025

## Class 3: Tool Calling

Invited [aaron@gauntlethq.com](mailto:aaron@gauntlethq.com) [Ash Tilawat](mailto:ash@gauntlethq.com)

Attachments [Class 3: Tool Calling](https://www.google.com/calendar/event?eid=NnNpbG1ocHF0MmlyMzcwdWZjMXJ0dXR2c3UgYXNoQGdhdW50bGV0aHEuY29t) [Class 3: Tool Calling - 2025/10/15 10:57 EDT - Recording](https://drive.google.com/file/d/1MTUMCuMpqqmUEkUHWxuI48eVhWtTS-9c/view?usp=drive_web) [Class 3: Tool Calling - 2025/10/15 10:57 EDT - Chat](https://drive.google.com/file/d/1ROivoIU3O9eNjK2j0cv__tlT_S0X6lRf/view?usp=drive_web) 

Meeting records [Transcript](?tab=t.a6494hnv0mdq) [Recording](https://drive.google.com/file/d/1MTUMCuMpqqmUEkUHWxuI48eVhWtTS-9c/view?usp=drive_web) 

### Summary

Aaron Gallant introduced language models (LLMs), distinguishing them from general AI and explaining their function as mathematical equations trained on statistical correlations of language, while also emphasizing that "hallucinations" are actually "confabulations." Aaron Gallant, Max Efremov, Kiran Rushton, Noah, Reuben, and Mike Tikh discussed tool calling as a method to constrain LLM output for reliability in engineering applications, with Aaron Gallant demonstrating its implementation using OpenAI and LangChain, and advocating for LangSmith for observability. Aaron Gallant advised participants to post questions in a Slack thread and summarized that the LangChain ecosystem, including LangGraph and LangSmith, is valuable for tool calling, emphasizing the use of tools for math problems and real-time data retrieval.

### Details

* **Introduction to Language Models and Technical Intuition** Aaron Gallant introduced themself as an instructor focusing on the technical and mathematical intuition behind language models (LLMs). They emphasized understanding these rapidly changing tools and the developing ecosystem, prioritizing intuition over hands-on mathematical details, but also noted the importance of practical application ([00:00:00](?tab=t.a6494hnv0mdq#heading=h.71msgy1iyay)). They highlighted that while the tools change quickly, the underlying mathematical concepts often have a longer history ([00:02:22](?tab=t.a6494hnv0mdq#heading=h.gicvi0clso9u)).

* **Distinguishing LLMs from AI and Understanding Their Function** Aaron Gallant discouraged anthropomorphizing LLMs, preferring terms like "language model" or "function" over "AI," which they viewed as a marketing term ([00:04:42](?tab=t.a6494hnv0mdq#heading=h.tpg0fggn26mc)). They explained that LLMs function like mathematical equations, processing language by converting it into numbers and back, and are trained on data to learn statistical correlations of language ([00:06:42](?tab=t.a6494hnv0mdq#heading=h.s9nt5gprhuo5)). They clarified that LLMs solve the "language problem," not "information retrieval," and their "hallucinations" are actually "confabulations" resulting from their normal mode of operation based on predicted tokens rather than truth ([00:07:42](?tab=t.a6494hnv0mdq#heading=h.puzg2hj0aacw)).

* **Tool Calling for Reliability and Constraint** Aaron Gallant introduced tool calling as a method to constrain the output of LLMs to fit a certain schema, which can then be used as arguments for functions. They emphasized that tool calling enhances the reliability of systems built with LLMs, making them more predictable and suitable for engineering applications where reliability is crucial ([00:09:46](?tab=t.a6494hnv0mdq#heading=h.uxypkfgmcegf)) ([00:11:51](?tab=t.a6494hnv0mdq#heading=h.n8mbyanpaiwf)). Tool calling allows LLMs to act as decision-makers and reasoning engines, influencing code execution ([00:12:52](?tab=t.a6494hnv0mdq#heading=h.cogembh9cmmo)).

* **Question and Answer Protocol** Aaron Gallant outlined the communication protocol for questions, instructing participants to post them in a Slack question thread due to the large group size, promising to pause periodically to address them. They also stated a preference for small group chats or office hours for unmuting and direct interaction ([00:03:29](?tab=t.a6494hnv0mdq#heading=h.249lh8i116gp)).

* **Tool Calling with OpenAI and LangChain** Aaron Gallant discussed implementing tool calling using OpenAI's API directly and with LangChain, a framework that provides "batteries included" features for building applications ([00:11:51](?tab=t.a6494hnv0mdq#heading=h.n8mbyanpaiwf)). They also introduced LangSmith, an observability platform specifically designed for LLM observability, which integrates well with LangChain ([00:12:52](?tab=t.a6494hnv0mdq#heading=h.cogembh9cmmo)).

* **Tool Classification and Problem Simplification** Aaron Gallant explained that tool calling can be viewed as a classification problem for the LLM, where it decides which tool to use next. They advised simplifying the problem by limiting the number of available tools, similar to how too many choices can overwhelm a human, to achieve more reliable output from the machine ([00:14:20](?tab=t.a6494hnv0mdq#heading=h.b6m8etufx724)).

* **LangChain Ecosystem Overview** Reuben inquired about the differences between LangChain, LangGraph, and LangSmith. Aaron Gallant clarified that LangChain is the parent organization, with LangGraph and LangSmith as distinct products under it; LangGraph and LangChain the framework are open source, while LangSmith is a hosted, monetized service ([00:15:28](?tab=t.a6494hnv0mdq#heading=h.gynyp4r8ixpp)).

* **Interaction with LLMs and Tool Calling** Max Efremov inquired about the connection between tool calling and CLI coding. Aaron Gallant explained that their CLI usage was simply a personal preference for running scripts and clarified that the example demonstrated how to build custom tool calling capabilities ([00:20:18](?tab=t.a6494hnv0mdq#heading=h.k4xg9dljykpl)) ([00:24:07](?tab=t.a6494hnv0mdq#heading=h.wfln9nc0vnms)). They further elucidated that while commercial LLM services like ChatGPT might abstract and hide their internal tool calling for features like web searching, the lecture teaches how to create arbitrary, custom tool calls for specific problems or data pipelines ([00:23:08](?tab=t.a6494hnv0mdq#heading=h.8rd0fpymc8nj)).

* **Best Practices for Tool Naming and Context Management** Kiran Rushton raised concerns about tool selection becoming problematic with many tools and the importance of descriptive naming ([00:24:56](?tab=t.a6494hnv0mdq#heading=h.gu3pcsnt5nu7)). Aaron Gallant advised against immediately resorting to advanced techniques like fine-tuning for tool selection, suggesting that thinking from a human engineer's perspective about readable code and concise documentation can be a reasonable starting point for LLMs ([00:26:03](?tab=t.a6494hnv0mdq#heading=h.f1xrlo7mxouw)). They also recommended specializing agents for complex classification problems, breaking down problems into smaller, more manageable tasks, and maintaining crisp tasks to avoid overly long functions ([00:27:11](?tab=t.a6494hnv0mdq#heading=h.7re17kg24j29)).

* **Demonstration of Tool Calling with OpenAI** Aaron Gallant demonstrated a basic tool call example using OpenAI's API, showcasing how an LLM returns a list of tool calls with arguments based on a user prompt ([00:29:50](?tab=t.a6494hnv0mdq#heading=h.ynhhiofibigk)). They explained that the system then executes these tool calls and appends their outputs to the message conversation before sending the full history back to the LLM for a final summary ([00:30:53](?tab=t.a6494hnv0mdq#heading=h.jl70mdyjf4wu)).

* **LangSmith for Traceability and Observability** Aaron Gallant detailed how LangSmith tracks and debugs interactions by making functions "traceable" using a decorator, allowing observation of execution order, input, and output of tool calls ([00:32:54](?tab=t.a6494hnv0mdq#heading=h.pi52d0dw5v8q)). They explained that LangSmith helps monitor token usage and costs, and also supports data annotation, dataset curation, and experiment management, allowing users to log feedback for runs ([00:35:26](?tab=t.a6494hnv0mdq#heading=h.cp4pplr4ecp0)) ([00:40:08](?tab=t.a6494hnv0mdq#heading=h.6oq8n6tg1noy)).

* **Nature of Tool Calling and LLM Non-Determinism** Noah inquired about how tool calling handles the non-deterministic nature of LLMs. Aaron Gallant explained that the tool specification acts as a filter on the LLM's output layer, ensuring that even though the LLM is inherently probabilistic, the tokens it generates are constrained to fit the defined schema ([00:42:31](?tab=t.a6494hnv0mdq#heading=h.2mvngxa7fauh)).

* **Model Quality and Practical Considerations** Reuben asked about the variability in output quality among different LLM models. Aaron Gallant stated that flagship models from major providers are generally competitive, with differences often coming down to whether a problem is well-established in the public domain ([00:45:00](?tab=t.a6494hnv0mdq#heading=h.89rhjcvss77c)). They suggested that pragmatic factors like cost and legal team approvals often drive the choice of model ([00:46:17](?tab=t.a6494hnv0mdq#heading=h.2pfxcxe9i5fj)).

* **Tool Usage and Evaluation Practices** Aaron Gallant clarified that LLMs are not forced to use a tool if the message is irrelevant and that they should decide to call a tool only if it benefits the input ([00:47:21](?tab=t.a6494hnv0mdq#heading=h.nxc4e6odv6io)). Regarding evaluation platforms, they emphasized the importance of labeled hold-out data to scientifically assess model performance iterations, a concept central to machine learning ([00:48:25](?tab=t.a6494hnv0mdq#heading=h.193e1sfb71os)). They also suggested that for large API responses, tools could be designed to summarize verbose outputs before returning them to the LLM, to manage context window bloat ([00:50:29](?tab=t.a6494hnv0mdq#heading=h.b3x4kwk4lsnw)).

* **LangChain Tool Calling Example with Real API** Aaron Gallant presented a LangChain tool calling example that required fewer lines of code and hit a real weather API, contrasting it with the earlier toy example. They highlighted that LangChain automates the generation of tool specifications from docstrings and type annotations ([00:51:26](?tab=t.a6494hnv0mdq#heading=h.kufk4qc7u4hu)). They also noted that tool call results are not always returned to the LLM, particularly if the function's purpose is a side effect like inserting data into a database ([00:53:47](?tab=t.a6494hnv0mdq#heading=h.4d03mx821uhg)).

* **Practical Application of LangChain Ecosystem** Aaron Gallant summarized that the LangChain ecosystem is valuable for incorporating tool calling into code due to its "batteries included" nature and pre-implemented tools and toolkits ([00:55:59](?tab=t.a6494hnv0mdq#heading=h.z58x5stc10s6)). They also highlighted the existence of a healthy community and opportunities for open-source contributions ([00:57:12](?tab=t.a6494hnv0mdq#heading=h.81550q14bei)).

* **Tools for Math Problems** Regarding math problems, Aaron Gallant strongly advised using a tool rather than trusting the LLM's native capabilities, as LLMs do not inherently perform calculations but rely on patterns from their training data. They explained that commercial LLMs likely use background tools for such tasks ([00:58:14](?tab=t.a6494hnv0mdq#heading=h.xvv3e6eq2kjb)).

* **Tool Use in API Calls vs. Chat Interface** Mike Tikh inquired whether tools are exclusively used in the chat interface or also during API calls. Aaron Gallant clarified that when writing code and interacting with the model as a function, users are just interacting with the model and not with the "magic" in the background, like a consumer product. He added that using tools is beneficial for math, handling side effects, real-time data retrieval, and well-defined structures ([00:59:05](?tab=t.a6494hnv0mdq#heading=h.9el6j7dd887j)).

* **Prompt Engineering and Best Practices for Invoking Functions/Tools** Aaron Gallant discussed prompt engineering, noting that while there's "voodoo" in the field, he prefers a descriptive approach when starting, often outlining a plan before execution ([01:00:05](?tab=t.a6494hnv0mdq#heading=h.m8d18semjzzg)). He also stated that "function call" and "tool call" are largely synonymous, both returning a structured specification for executing code. He emphasized that best practices for invocation depend on the application and domain, particularly considering legal or privacy ramifications and suggesting human review or static analysis for critical applications ([01:00:57](?tab=t.a6494hnv0mdq#heading=h.770uxpjzq1s)).

* **Meeting Conclusion** Aaron Gallant announced that he had to leave but would follow up on questions in Slack, as the Google chat would be lost ([01:00:05](?tab=t.a6494hnv0mdq#heading=h.m8d18semjzzg)). He thanked everyone and stated that he would be back to discuss agents. Brian Smith confirmed that there was nothing else scheduled after the meeting ([01:02:00](?tab=t.a6494hnv0mdq#heading=h.vpgmeocekpmu)).

### Suggested next steps

- [ ] Aaron Gallant will start a question thread in the Slack and keep an eye on it to answer questions.  
- [ ] Aaron Gallant will follow up on more questions in the thread and about error management in Slack.

*You should review Gemini's notes to make sure they're accurate. [Get tips and learn how Gemini takes notes](https://support.google.com/meet/answer/14754931)*

*Please provide feedback about using Gemini to take notes in a [short survey.](https://google.qualtrics.com/jfe/form/SV_9vK3UZEaIQKKE7A?confid=_BEfp3r_o7KrW01J062HDxISOAIIigIgABgBCA&detailid=unspecified)*