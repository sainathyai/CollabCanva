AI DEVELOPMENT LOG

Project: CollabCanvas - Real-Time Collaborative Canvas
Developer: Sainath Yatham
AI Tool: Cursor (Claude Sonnet 4.5)
Timeline: October 13-14, 2025 (~24 hours)
Live Demo: https://collab-canva-jdte.vercel.app
Repository: https://github.com/sainathyai/CollabCanva

================================================================================

1. TOOLS & WORKFLOW

I used Cursor IDE with Claude Sonnet 4.5 for 100% of code generation. The entire
development followed a conversational, iterative approach.


THE STRATEGIC PLANNING PHASE (MOST CRITICAL)

This was the most important part of the entire project. Before writing a single
line of code, I spent time with AI to:

Generate a Focused PRD
  • Started with a complex project brief describing a week-long project
  • Prompted AI to extract ONLY the 24-hour MVP requirements
  • Result: Clear PRD with 10 acceptance criteria, technical stack, and explicit
    "out of scope" section
  • This prevented scope creep throughout development

Create Detailed Task Breakdown
  • AI generated 9 Pull Requests with exact file paths
  • Each PR had specific subtasks like "Create backend/src/auth/verifyToken.ts"
  • Not just "implement auth" but precise file-level instructions
  • This roadmap guided the entire execution phase

Design System Architecture
  • AI created Mermaid diagrams showing component relationships
  • Defined message protocol (WebSocket message types)
  • Chose technical patterns (singleton WebSocket client, broadcast pattern)
  • Had a clear mental model before coding

Why This Mattered:
The 1 hour spent planning saved 5+ hours of rework. Every subsequent PR referenced
this roadmap. No time wasted on "what should I build next?" or "how does this fit
together?"


EXECUTION PHASE

Once planning was done, execution was straightforward:

PR-by-PR Development
  • Worked through PR1 → PR9 in order
  • Simple prompt: "work on pr1 of the tasks only"
  • AI read Tasks.md and generated complete files
  • Each PR was merge-ready with no placeholders

Real-Time Problem Solving
  • Hit 12+ deployment errors (TypeScript, build configs, auth issues)
  • Pattern: Copy error log → Paste to AI → Get fix → Test → Move on
  • ~90% success rate on first fix attempt
  • Fast iteration without fatigue or frustration

Documentation
  • AI generated README, architecture docs, this development log
  • Written alongside code, not as afterthought
  • Better quality than typical human-written docs (consistent, thorough)


WHAT I ACTUALLY DID MANUALLY

The Memory Bank system (6 files with architecture patterns, decisions, progress)
was created at the end of the project, not during MVP development. It's for future
sessions and maintenance, not for this 24-hour sprint.

Git Workflow
  • AI executed all Git commands (init, branch, commit, push, merge)
  • AI wrote commit messages following conventional format
  • I manually created PRs on GitHub (clicked the button)
  • AI resolved merge conflicts when integrating PR7

Deployment
  • AI configured Vercel (frontend) and Render (backend) via config files
  • AI debugged build failures by reading error logs
  • I manually set environment variables in dashboards
  • I manually added SSH keys to GitHub

================================================================================

2. PROMPTING STRATEGIES

Here are 5 prompts that worked really well:


PROMPT 1: SCOPE REDUCTION (INITIAL PLANNING)

"Create a formal PRD based on the attached project plan. Your primary task is to
strictly limit scope to ONLY the MVP requirements achievable within 24 hours."

Why it worked:
Clear time constraint forced pragmatic decisions. AI suggested in-memory state
instead of database, focused on core features only. Result was 10 acceptance
criteria instead of 20+ features.


PROMPT 2: FILE-LEVEL TASK BREAKDOWN

"Generate a task list broken down by Pull Requests. For each subtask, specify
the EXACT file paths that will be created or edited."

Why it worked:
No ambiguity. Not "implement WebSocket" but "Create backend/src/ws/handlers.ts
with message routing." Made each PR actionable and concrete.


PROMPT 3: INCREMENTAL EXECUTION

"work on the pr1 of the tasks only"

Why it worked:
Simple, direct. AI read Tasks.md context and stayed focused. Prevented scope creep.
Repeated this pattern for all 9 PRs with consistent results.


PROMPT 4: ERROR-DRIVEN DEBUGGING

"Build failed src/ws/handlers.ts(1,27): error TS7016: Could not find a
declaration file for module 'ws'. [full error log]"

Why it worked:
Actual error output (not paraphrased) let AI diagnose root cause. Identified that
NODE_ENV=production was blocking devDependencies. Fixed in one attempt after
seeing full context.


PROMPT 5: STRATEGIC QUESTIONS

"Why are you cd'ing every time even though you are in the current project root?"

Why it worked:
Direct feedback improved AI behavior. After this, AI checked current directory
before running commands. Human oversight refined the workflow.

================================================================================

3. CODE ANALYSIS


OVERALL SPLIT

  • AI Generated: ~95%
  • Human Written: ~5%


WHAT AI GENERATED

Component             AI %    Details
-----------------------------------------------------------------------
Backend code          98%     Entire WebSocket server, auth, state management
Frontend code         98%     React components, WebSocket client, canvas logic
Config files          100%    tsconfig, vite.config, vercel.json, render.yaml
Documentation         100%    README, PRD, Tasks, Architecture docs, this log
Git operations        95%     All commands, commit messages, conflict resolution


WHAT I ACTUALLY WROTE

The 5% human contribution was:
  • Environment values: Firebase API keys, project IDs (sensitive data)
  • Platform actions: Added SSH keys to GitHub, set env vars in dashboards
  • Testing: Opened 2 browsers, signed in with different Google accounts
  • Decisions: Chose Firebase over Okta, confirmed deployment domains


HOW AI GENERATED CODE

AI didn't write snippets - it wrote complete files from specification:
  • Prompt: "Create backend/src/ws/handlers.ts with message routing"
  • AI output: 300-line file with imports, types, handlers, error handling, logging
  • Pattern repeated for all 40+ source files
  • No placeholders like "// TODO: implement this"

================================================================================

4. STRENGTHS & LIMITATIONS


WHERE AI EXCELLED

Architecture & Design
  • Created system architecture with Mermaid diagrams
  • Designed WebSocket message protocol with TypeScript enums
  • Chose appropriate patterns (singleton, broadcast, optimistic UI)
  • Has breadth of knowledge across many tech stacks

Boilerplate & Config
  • Perfect tsconfig.json, vite.config.ts, package.json configs
  • Correct TypeScript types for all interfaces
  • Proper ESM module setup without trial-and-error
  • Knows correct patterns from seeing millions of examples

Documentation
  • Professional README with quickstart, architecture, deployment
  • Comprehensive docs (8,000+ lines of markdown)
  • Consistent format, no laziness
  • Better than most human-written project docs

Debugging
  • Fixed 12+ deployment errors by analyzing logs
  • Identified subtle issues (NODE_ENV blocking devDependencies, path issues)
  • Fast iteration without fatigue
  • ~90% first-fix success rate

Full-Stack Consistency
  • Kept frontend/backend message types in sync
  • Consistent error handling patterns
  • Matching interfaces between client and server
  • Sees entire codebase, prevents type mismatches

Git Workflow
  • Correct Git commands every time
  • Meaningful commit messages
  • Resolved merge conflicts during PR7
  • Consistent hygiene without human forgetfulness


WHERE AI STRUGGLED

Platform UIs
  • Couldn't add SSH keys to GitHub (manual copy/paste needed)
  • Couldn't set environment variables in Vercel/Render dashboards
  • Couldn't create GitHub PRs (pushed branch, I clicked button)
  • Limited to terminal and file operations

External Services
  • Needed me to verify Firebase project ID
  • Needed me to add authorized domains in Firebase Console
  • Needed me to confirm Google OAuth was enabled
  • Can't access external dashboards

Ambiguity
  • "Authentication not working" → needed specific error logs
  • "Deployment failed" → needed actual build output
  • Can't "see" browser console or network tab
  • Needs precise information, not descriptions


WHAT AI NEEDED FROM ME

1. Credentials (API keys, project IDs, secrets)
2. Manual platform actions (dashboard configurations)
3. Testing and validation ("yes, cursors are working")
4. Strategic decisions ("use Firebase" vs "use Okta")
5. Network/environment info (local machine setup)

================================================================================

5. KEY LEARNINGS


LEARNING 1: AI THRIVES ON CONSTRAINTS

The best prompt was the first one - "24-hour MVP only." Without constraints, AI
generates ideal solutions (over-engineered). With constraints, AI optimizes
within boundaries (pragmatic).

Actionable: Always specify time, scope, and complexity constraints upfront.


LEARNING 2: PLANNING BEATS PERFECT CODE

The 1-hour planning phase (PRD + Tasks + Architecture) saved 5+ hours during
execution. Having a roadmap meant no time wasted on "what's next?" or "how does
this fit?"

Actionable: Invest in strategic planning with AI before coding. Generate PRD and
detailed task breakdown first.


LEARNING 3: ERROR LOGS ARE GOLD

12 deployment errors fixed with ~90% success rate. The pattern that worked:

  Me: "Build failed [paste full error log]"
  AI: [Analyzes] → [Proposes fix] → [Implements] → [Commits]
  Result: Fixed

The pattern that failed:

  Me: "Authentication doesn't work"
  AI: [Asks clarifying questions] → More info → [Repeat 3x]
  Result: Slow

Actionable: Always provide error logs, console output, specific symptoms. No
paraphrasing.


LEARNING 4: AI BETTER AT "SOLVED PROBLEMS"

Configuration files, boilerplate, standard patterns = 100% AI-generated. AI has
seen millions of examples of:
  • TypeScript config for Vite + React
  • WebSocket server setup
  • Firebase Auth integration
  • Vercel/Render deployment configs

Human time better spent on novel business logic, UX decisions, testing edge cases.

Actionable: Let AI generate all boilerplate/config. Focus human effort on unique
problems.


LEARNING 5: ITERATE, DON'T PERFECT

Best results came from conversational iteration, not one-shot prompts.

Example (Deployment Debugging):
  Prompt 1: "Deploy to Render" → AI creates render.yaml
  Prompt 2: "Build failed [error]" → AI fixes TypeScript types
  Prompt 3: "Still failing [error]" → AI fixes build command
  Prompt 4: "Start command wrong" → AI corrects path
  Result: Working deployment after 4 iterations

AI can't predict all edge cases upfront. Rapid iteration beats upfront perfection.

Actionable: Start with clear goal, iterate based on results. Don't spend 30
minutes crafting perfect prompt.


LEARNING 6: HUMAN ROLE SHIFTED

Traditional flow: Human writes code → AI reviews → Human fixes
AI-first flow: Human specifies goal → AI generates → Human tests → Iterate

I never said "rewrite this function differently." Instead:
  • "Authentication works? Let's move to PR6"
  • "Cursors not showing? [console log]"
  • "Great, now deploy to production"

Human role is product validator, not code reviewer.

Actionable: Structure workflow around behavioral validation, not line-by-line
code review.


LEARNING 7: DOCUMENTATION COMPOUNDS

AI-generated docs alongside code (not after) improved code quality. Writing "why
WebSocket over HTTP polling" in docs solidified the pattern. Subsequent PRs
referenced earlier docs for consistency.

Actionable: Generate docs alongside code. Don't treat as post-project cleanup.

================================================================================

6. RESULTS & METRICS


TIMELINE BREAKDOWN

  • Planning: 1 hour (PRD, task breakdown, architecture)
  • Development: ~18 hours (PR1 through PR9)
  • Deployment debugging: ~3 hours (fixing build errors)
  • Documentation: ~2 hours (Memory Bank, final docs)
  • TOTAL: ~24 hours from start to deployed MVP


CODE VOLUME GENERATED

  • Frontend: ~2,500 lines (TypeScript/React)
  • Backend: ~1,800 lines (TypeScript/Node.js)
  • Config files: ~500 lines (tsconfig, vite, vercel, render)
  • Documentation: ~8,000 lines (markdown)
  • TOTAL: ~12,800 lines in 24 hours


GIT ACTIVITY

  • 9 feature PRs (all planned upfront)
  • 47 commits (all AI-generated messages)
  • 0 reverted commits (no major mistakes)
  • 1 merge conflict resolved (PR7 integration)


DEPLOYMENT SUCCESS

All 10 acceptance criteria met:

AUTHENTICATION (AC-A1, AC-A2)
  ✓ Users sign in with Google OAuth
  ✓ Display names visible in UI

CANVAS (AC-C1, AC-C2)
  ✓ Two users see same canvas session
  ✓ Users create/move objects, both clients sync

REAL-TIME SYNC (AC-RS1, AC-RS2)
  ✓ Actions appear instantly for other users
  ✓ No actions lost or duplicated

PRESENCE (AC-P1, AC-P2)
  ✓ Users see each other's live cursors
  ✓ Cursors display correct names

DEPLOYMENT (AC-D1, AC-D2)
  ✓ Public staging URL works with 2+ users
  ✓ GitHub repository publicly accessible

PRODUCTION URLS:
  • Frontend: https://collab-canva-jdte.vercel.app (Vercel)
  • Backend: https://collabcanva-backend.onrender.com (Render)
  • Repository: https://github.com/sainathyai/CollabCanva

================================================================================

7. CONCLUSION


WAS AI-FIRST DEVELOPMENT SUCCESSFUL?

Yes, overwhelmingly.

The project achieved:
  • Full MVP in 24 hours (all 10 criteria met)
  • Production deployment on free tier hosting
  • Comprehensive documentation
  • Zero technical debt (clean architecture, typed codebase)
  • Working live demo with real-time collaboration


TIME SAVED

Could this be built without AI? Yes, but not in 24 hours by one person.

What would have taken much longer:
  • Boilerplate setup (tsconfig, vite, routing): ~3 hours saved
  • WebSocket implementation (connection handling): ~4 hours saved
  • Deployment configuration (trial-and-error): ~2 hours saved
  • Documentation (README, architecture): ~4 hours saved
  • Debugging (TypeScript, deployment): ~2 hours saved

ESTIMATED: 24 hours with AI vs. ~40 hours without


RECOMMENDATIONS

Use AI for:
  • All boilerplate and configuration
  • System architecture design
  • Documentation generation
  • Debugging with error logs
  • Standard patterns (auth, WebSocket, APIs)

Use Humans for:
  • Strategic decisions (tech stack choices)
  • User experience validation
  • Manual testing
  • External service configuration
  • Credential management

Best approach: AI-first with human oversight, not AI-only or human-only.

================================================================================

PROJECT RESOURCES

LIVE APPLICATION
  • Frontend: https://collab-canva-jdte.vercel.app
  • Backend: https://collabcanva-backend.onrender.com

REPOSITORY
  • GitHub: https://github.com/sainathyai/CollabCanva

KEY DOCUMENTATION
  • PRD: docs/PRD_MVP.md (10 acceptance criteria)
  • Architecture: docs/ARCHITECTURE_DETAILED.md (3 Mermaid diagrams)
  • Tasks: docs/Tasks.md (9 PRs, file-level breakdown)
  • Deployment: docs/DEPLOYMENT_SUMMARY.md
  • Memory Bank: memory-bank/ (6 files for future AI sessions)

KEY TECHNICAL FILES
  • Frontend: frontend/src/pages/Canvas.tsx (main app logic)
  • Backend: backend/src/ws/handlers.ts (WebSocket routing)
  • Auth: backend/src/auth/verifyToken.ts (Firebase verification)

================================================================================

Developer: Sainatha Yatham
Date: October 14, 2025
AI Tool: Cursor (Claude Sonnet 4.5)
Development Approach: AI-First with Human Validation

